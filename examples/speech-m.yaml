# This config file replicates the Listen-Attend-Spell architecture: https://arxiv.org/pdf/1508.01211.pdf
# Compared to the conventional attentional model, we remove input embeddings, instead directly read in a feature vector
# the pyramidal LSTM reduces length of the input sequence by a factor of 2 per layer (except for the first layer).
# Output units should be characters according to the paper.
defaults:
  experiment:
    model_file: ../examples/output/<EXP>.mod
    hyp_file: ../examples/output/<EXP>.hyp
    out_file: ../examples/output/<EXP>.out
    err_file: ../examples/output/<EXP>.err
    run_for_epochs: 2
    decode_every: 1
    eval_metrics: cer,wer
  train:
    train_src: ../examples/data/synth.contvec.npz
    train_trg: ../examples/data/synth.char
    dev_src: ../examples/data/synth.contvec.npz
    dev_trg: ../examples/data/synth.char
    # choose pyramidal LSTM encoder:
    encoder:
      type: PyramidalBiLSTM
    # indicate the dimension of the feature vectors:
    input_word_embed_dim:
    - 80
    - 3
    # indicates that the src-side data is continuous-space vectors, contained in a numpy archive (see input.py for details):
    input_format: contvec
    neighbor_label_smoothing_weights:
    - 0.9
    - 0.05
  decode:
    src_file: ../examples/data/synth.contvec.npz
    input_format: contvec
  evaluate:
    ref_file: ../examples/data/synth.char

speech:
  train:
    default_layer_dim: 512
    attender_hidden_dim: 128
    output_word_embed_dim: 64
    output_state_dim: 256
    output_mlp_hidden_dim: 512
    decoder_layers: 2
    encoder:
      type: modular
      modules:
#      - type: StridedConv
#        layers: 3
#        output_tensor: False
#        input_dim: 240
#        chn_dim: 3
#        num_filters: 32
#        batch_norm: True
#        nonlinearity: maxout
#        stride:
#        - (2,2)
#        - (2,2)
#        - (2,1)
      - type: PoolingConv
        input_dim: 240
        pooling:
        - False
        - (2,2)
        - False
        - (2,2)
#      - type: ConvBiLSTM
#        layers: 1
#        chn_dim: 32
#        num_filters: 32
#        input_dim: 544
#        residual: True
#      - type: NiNBiLSTM
#        layers: 2
#        input_dim: 1088
#        batch_norm: True
#        stride: 1
#        num_projections: 1
#        projection_enabled: True
#        nonlinearity: relu
      - type: BiLSTM
        layers: 1
        input_dim: 512
        output_dim: 512
        

