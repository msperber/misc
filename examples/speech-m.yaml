# This config file replicates the Listen-Attend-Spell architecture: https://arxiv.org/pdf/1508.01211.pdf
# Compared to the conventional attentional model, we remove input embeddings, instead directly read in a feature vector
# the pyramidal LSTM reduces length of the input sequence by a factor of 2 per layer (except for the first layer).
# Output units should be characters according to the paper.
defaults:
  experiment:
    model_file: ../examples/output/<EXP>.mod
    hyp_file: ../examples/output/<EXP>.hyp
    out_file: ../examples/output/<EXP>.out
    err_file: ../examples/output/<EXP>.err
    run_for_epochs: 2
    decode_every: 1
    eval_metrics: cer,wer
  train:
    train_src: ../examples/data/synth.contvec.npz
    train_trg: ../examples/data/synth.char
    dev_src: ../examples/data/synth.contvec.npz
    dev_trg: ../examples/data/synth.char
    # choose pyramidal LSTM encoder:
    encoder:
      type: PyramidalBiLSTM
    # indicate the dimension of the feature vectors:
    input_word_embed_dim: 240
    # indicates that the src-side data is continuous-space vectors, contained in a numpy archive (see input.py for details):
    input_format: contvec
  decode:
    src_file: ../examples/data/synth.contvec.npz
    input_format: contvec
  evaluate:
    ref_file: ../examples/data/synth.char

speech:
  train:
    default_layer_dim: 512
    encoder:
      type: modular
      modules:
      - type: StridedConv
        layers: 2
        output_tensor: True
        input_dim: 240
        chn_dim: 3
        num_filters: 32
      - type: ConvBiLSTM
        layers: 1
        chn_dim: 32
        input_dim: 608
      - type: NiNBiLSTM
        layers: 2
        input_dim: 1216
        output_dim: 512
      - type: BiLSTM
        layers: 1
        input_dim: 512
        output_dim: 512
        

