# This config file replicates the Listen-Attend-Spell architecture: https://arxiv.org/pdf/1508.01211.pdf
# Compared to the conventional attentional model, we remove input embeddings, instead directly read in a feature vector
# the pyramidal LSTM reduces length of the input sequence by a factor of 2 per layer (except for the first layer).
# Output units should be characters according to the paper.
defaults:
  experiment:
    model_file: examples/output/<EXP>.mod
    hyp_file: examples/output/<EXP>.hyp
    out_file: examples/output/<EXP>.out
    err_file: examples/output/<EXP>.err
    run_for_epochs: 20
    decode_every: 1
    eval_metrics: cer,wer
  train:
    train_src: examples/data/synth.contvec.npz
    train_trg: examples/data/synth.char
    dev_src: examples/data/synth.contvec.npz
    dev_trg: examples/data/synth.char
    # choose pyramidal LSTM encoder:
    encoder:
      type: PyramidalBiLSTM
      downsampling_method: skip
    # indicate the dimension of the feature vectors:
    input_word_embed_dim: 240
    # indicates that the src-side data is continuous-space vectors, contained in a numpy archive (see input.py for details):
    input_format: contvec
  decode:
    src_file: examples/data/synth.contvec.npz
    input_format: contvec
  evaluate:
    ref_file: examples/data/synth.char

speech:
  train:
    encoder:
      layers: 3
      hidden_dim: 64
      downsampling_method: concat
    output_word_embed_dim: 64
    output_state_dim: 64
    attender_hidden_dim: 64
    output_mlp_hidden_dim: 64
    attention_context_dim: 64

