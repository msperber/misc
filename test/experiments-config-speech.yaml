# This config file replicates the Listen-Attend-Spell architecture: https://arxiv.org/pdf/1508.01211.pdf
# Compared to the conventional attentional model, we remove input embeddings, instead directly read in a feature vector
# the pyramidal LSTM reduces length of the input sequence by a factor of 2 per layer (except for the first layer).
# Output units should be characters according to the paper.
defaults:
  experiment:
    model_file: /tmp/model.out
    hyp_file: /tmp/xnmt-hypothesis
    run_for_epochs: 2
    decode_every: 1
    eval_metrics: cer,wer
  train:
    train_source: ../data/synth.contvec.npz
    train_target: ../data/synth.char
    dev_source: ../data/synth.contvec.npz
    dev_target: ../data/synth.char
    # choose pyramidal LSTM encoder:
    encoder_type: PyramidalBiLSTM
    # indicate the dimension of the feature vectors:
    input_word_embed_dim: 240
    # indicates that the source-side data is continuous-space vectors, contained in a numpy archive (see input.py for details):
    input_format: contvec
  decode:
    source_file: ../data/synth.contvec.npz
    input_format: contvec
  evaluate:
    ref_file: ../data/synth.char

experiment1:
  train:
    encoder_layers: 3
    output_word_embed_dim: 64
    output_state_dim: 64
    attender_hidden_dim: 64
    output_mlp_hidden_dim: 64
    encoder_hidden_dim: 64

